{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1\n",
    "\n",
    "## 1. Review of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Name|Model|Advantages (Not limited to)|Shortcomings (In our context)|\n",
    "|:---|:----|:--------------------------|:----------------------------|\n",
    "| **Survival analysis** | *Survival analysis* is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems.<br>**Typical Models**: accelerated failure time (AFT) models.<br>**Details**: AFT models incorporate covariates $X\\in \\mathcal{R}^d$ into the survival function $S(t)=P(T>t)$, which are equivalent to log-linear models of time $t$. $S(t|\\beta,X)=S_0(\\exp(\\beta^T X)\\cdot t)$ Under this setting, Weibull distribution is always chosen to be the baseline survival function ($S_0\\sim\\text{Weibull}(S_0:\\lambda,k)$), making the log-linear error distribution a Gumbel (extreme value) distribution ($\\epsilon\\sim\\text{Gumbel}(\\epsilon:\\mu,\\gamma)$).<br>**Note**: A typical Bayesian setting introduces priors to both coefficients of additive features and the hyperparameter of the Gumbel distribution. For example, we can pick independent, vague normal priors. $\\beta\\sim \\text{Normal}(0,\\sigma_1^2 I^d)$, $\\mu=0$, $\\gamma\\sim \\text{Half-Normal}(0,\\sigma_2^2)$. | - Capture the characteristics of many applications.<br>- Correctly incorporate information from both censored and uncensored observations in estimating important model parameters. (\"censoring\": only gather data before the \"death\" of the event) | - Work only efficiently with representative features that are wisely engineered.<br>- Need effective models to back up: the original data generated as from different sensors may have been simulated under complex physics or material science principles. With 26 features in total, this is nearly impossible for us to capture the true relationships.<br>- Lack explanatory of features for cumbersome models. |\n",
    "| **Churn prediction** | *Churn prediction* predicts whether customers are about to leave(non-event prediction, *churn score*). This can be viewed as a *machinelearning* problem.<br>**Typical models**: GLM, SVM, k-NN, Neural Network and so on.<br>**Details**: A typical Recurrent Neural Network may capture such recurrent and time-variant structure, for each time step from $t=1$ to $t=\\tau$, $\\mathbf{a}^{(t)}=\\mathbf{b}+\\mathbf{Wh}^{(t-1)}+\\mathbf{Ux}^{(t)}$,$\\mathbf{h}^{(t)}=\\text{tanh}(\\mathbf{a}^{(t)})$  $\\mathbf{o}^{(t)}=\\mathbf{c}+\\mathbf{Vh}^{(t)}$,$\\hat{\\mathbf{y}}^{(t)}=\\text{softmax}(\\mathbf{o}^{(t)})$ Bias vectors $\\mathbf{b}$ and $\\mathbf{c}$, Weight matrices $\\mathbf{U}$(input-to-hidden), $\\mathbf{V}$ (hidden-to-output) and $\\mathbf{W}$ (hidden-to-hidden) We train the model using the defined loss function such as log-likelihood loss.<br>**Note**: Although in churn analysis the predictive task is different fromestimating the survival function, it is worth noticing that predicting theevent status, given the individuals have survived up to the specified time, is essentially related with hazard function, which is also part of the survival analysis. | - Straightforward as the predictive goal (in a narrower definition of inference) is clearly defined with enough number of features.<br>- Tons of models | - Similar to downsides of survival analysis.<br>- Doesn’t allow flexibility of model parameters (no uncertainty) |\n",
    "| **Churn prediction** (probabilistic) | A probabilistic setup uses the mathematics ofprobability theory to express all forms of uncertainty and noise associatedwith our model.<br>**Typical Models**: Gaussian Mixture Model Expectation-Maximization methodto approximate the mean and covariance.<br>**Details**:see [Lin et al., 2013](http://www.sciencedirect.com/science/article/pii/S095183201300149X) | - Approximate any arbitrary distribution with a mixture model.<br>- Simplify the representation of the model framework. | - Doesn’t know GMM’s number of clusters<br>- Assume an underlying Gaussian generative distribution, which may not be the case for our dataset (lack explanatory of clusters) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Nets\n",
    "\n",
    "**simple RNN**, the structure is as follows:\n",
    "<img src=\"../src/out/rnn.jpg\" alt=\"A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature\" style=\"width: 500px;\"/>\n",
    "Simple RNN is good at remembering previous information. It will integrate data from previous period into its hidden state, and propagate the hidden state to the next period. We will also use hyperbolic tangent activation in the hidden layer.  \n",
    "\n",
    "**LSTM** network contains a (memory) cell and three gates that control or regulate information flow: an input gate, an output gate and a forget gate. These gates compute an activation often using the logistic function. These gates can be thought as conventional artificial neurons. The cell \"remembers\" a value for either long or short time periods. The input gate controls the extent to which a new value flows into the memory, the forget gate controls the extent to which a value remains in memory and the output gate controls the extent to which the value in memory is used to compute the output activation of the LSTM block.  \n",
    "<img src=\"../src/out/LSTM.png\" alt=\"LSTM Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)\" style=\"width: 300px;\"/>\n",
    "\n",
    "Gated recurrent unit (**GRU**) is a gating mechanism that is similar to that of long short-term memory. They have fewer parameters than LSTM, as they lack the cell and the output gate, but they have similar performance in many cases.  \n",
    "<img src=\"../src/out/GRU.png\" alt=\"GRU Gating. Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” (2014)\" style=\"width: 300px;\"/>\n",
    "In many cases, stacked RNNs may get better results since they have deeper structures:\n",
    "<img src=\"../src/out/stackedRNN.png\" alt=\"stackedRNN\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
