{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 2: Experimentation on Bayes by Backprop\n",
    "\n",
    "Nov 30 Update\n",
    "\n",
    "Bug description:\n",
    "\n",
    "- In code chunk 9, when we try to build a network to accept the preprocessed inputs, we fail at the `outputs, state = tf.nn.dynamic_rnn()` step.\n",
    "- We have experimented two ways of building a recurrent net with different layer sizes:\n",
    "  - `tf.nn.dynamic_rnn()`: able to build a graph with no dimension issue, but fail at 'Merge/MergeSummary' operation for hyperparameters ([similar issue 1](https://stackoverflow.com/questions/43513417/tensorflow-how-to-add-summary-into-dynamic-rnn-cell), [similar issue 2](https://github.com/tensorflow/tensorflow/issues/8660))\n",
    "  - Manually forward pass for the truncated minibatch\n",
    "    ```\n",
    "    outputs = []\n",
    "    with tf.variable_scope(\"RNN\"):\n",
    "        for time_step in range(seq_length):\n",
    "            if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "            (cell_output, state) = multi_rnn_cell(X[:, time_step, :], state)\n",
    "            outputs.append(cell_output)\n",
    "    outputs = tf.reshape(tf.concat(outputs, 1), [-1, layers[-1]])\n",
    "    ```\n",
    "    This will throw a dimension error that won't concatenate previous units to current layer weights properly.\n",
    "- We may try to experiment with same layer size afterwards as the tutorial, which won't throw an error as we described. But this shall still fall into an open issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicRNNCell\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.contrib.rnn import MultiRNNCell\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "from tensorflow.contrib.distributions import Normal\n",
    "\n",
    "from data_preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "\n",
    "# Parameters for priors\n",
    "pi = 0.5\n",
    "rho_1 = 1.0\n",
    "rho_2 = 0.5\n",
    "sigma_1 = tf.nn.softplus(rho_1) + 1e-8\n",
    "sigma_2 = tf.nn.softplus(rho_2) + 1e-8\n",
    "sigma_mix = np.sqrt(0.5 * np.square(1.) + (1. - 0.5) * np.square(0.5))\n",
    "\n",
    "# Parameters for network structure\n",
    "layers = [150, 50]\n",
    "n_layer = len(layers)\n",
    "batch_size = 200\n",
    "seq_length = 5\n",
    "n_epoch = 30\n",
    "n_feature = 25\n",
    "inference_mode = 'sample'\n",
    "bias = True\n",
    "\n",
    "# Parameters for learning rate and optimization\n",
    "init_scale = 0.1 # initial randomization scale\n",
    "max_grad_norm = 5 # gradient clipping by a global norm\n",
    "learning_rate = 0.01\n",
    "lr_decay = 0.9\n",
    "n_epoch_decay = 15 # learning rate decay after 'n_epoch_decay' epochs\n",
    "\n",
    "# Parameters for operations\n",
    "save_path = './model/saved_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def log_gaussian(x, mu, sigma):\n",
    "    return -.5 * np.log(2. * np.pi) - tf.log(tf.abs(sigma)) - \\\n",
    "        (x - mu) ** 2 / (2. * sigma ** 2)\n",
    "\n",
    "    \n",
    "def log_gaussian_(x, mu, rho):\n",
    "    return -.5 * np.log(2. * np.pi) - rho / 2. - (x - mu) ** 2 / \\\n",
    "        (2. * tf.exp(rho))\n",
    "\n",
    "    \n",
    "def KL_scale_mixture(shape, mu, sigma, prior, w):\n",
    "    \"\"\"Compute KL for scale mixture Gaussian priors\n",
    "    shape = (n_unit, n_w)\n",
    "    \"\"\"\n",
    "    posterior = Normal(mu, sigma)\n",
    "    part_post = posterior.log_prob(tf.reshape(w, [-1])) # flatten\n",
    "    prior_1 = Normal(0., prior.sigma_1)\n",
    "    prior_2 = Normal(0., prior.sigma_2)\n",
    "    part_1 = tf.reduce_sum(prior_1.log_prob(w)) + tf.log(prior.pi)\n",
    "    part_2 = tf.reduce_sum(prior_2.log_prob(w)) + tf.log(prior.pi)\n",
    "    prior_mix = tf.stack([part_1, part_2])\n",
    "    KL = - tf.reduce_sum(tf.reduce_logsumexp(prior_mix, axis=0)) + \\\n",
    "        tf.reduce_sum(part_post)\n",
    "    return KL\n",
    "\n",
    "\n",
    "def pm_producer(X, Y, batch_size, name=None):\n",
    "    \"\"\"Iterate on the raw data.\n",
    "\n",
    "    This chunks up raw_data into batches of examples and returns Tensors that\n",
    "    are drawn from these batches.\n",
    "\n",
    "    Args:\n",
    "    X, Y: the raw data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    name: the name of this operation (optional).\n",
    "\n",
    "    Returns:\n",
    "    A pair of Tensors:\n",
    "    The first: each shaped [batch_size, seq_length, n_feature].\n",
    "    The second element is the same data time-shifted to the right by one.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"PMProducer\", [X, Y, batch_size]):\n",
    "        X = tf.convert_to_tensor(X, name=\"X\", dtype=tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, name=\"Y\", dtype=tf.float32)\n",
    "\n",
    "\n",
    "    data_len = tf.size(Y)\n",
    "    batch_len = data_len // batch_size\n",
    "    X = tf.slice(X,[0,0,0],[batch_size*batch_len,-1,-1])\n",
    "    Y = tf.slice(Y,[0],[batch_size*batch_len])\n",
    "\n",
    "    i = tf.train.range_input_producer(batch_len, shuffle=False).dequeue()\n",
    "    x = tf.slice(X, [i * batch_size, 0, 0], [(i + 1) * batch_size, -1, -1])\n",
    "    x.set_shape([batch_size, seq_length, n_feature])\n",
    "    y = tf.slice(Y, [i * batch_size], [(i + 1) * batch_size])\n",
    "    y.set_shape([batch_size])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preprocessing dataset...\")\n",
    "\n",
    "# Load data\n",
    "train_df, test_df= load_and_preprocess(download=False)\n",
    "\n",
    "# Build datasets based on the given configuration\n",
    "X_train, y_train = config_data(train_df, seq_length, batch_size)\n",
    "X_test, y_test = config_data(test_df, seq_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batch_train = X_train.shape[0] // batch_size\n",
    "n_batch_test = X_test.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BayesianLSTM(BasicLSTMCell):\n",
    "    \"\"\"Build Bayesian LSTM layer with given configuration\n",
    "    1. Inherit from tensorflow BasicLSTMCell\n",
    "    2. Only difference in replacing with noisy weights\n",
    "    \"\"\"\n",
    "    def __init__(self, n_unit_pre, n_unit, prior, is_training, inference_mode, bias=True,\n",
    "                 forget_bias=1.0, state_is_tuple=True, activation=tf.tanh,\n",
    "                 reuse=None, name=None):\n",
    "        super().__init__(n_unit, forget_bias, state_is_tuple, activation,\n",
    "                                             reuse=reuse)\n",
    "\n",
    "        self.prior = prior\n",
    "        self.bias = bias\n",
    "        self.is_training = is_training\n",
    "        self.n_unit = n_unit\n",
    "        self.n_unit_pre = n_unit_pre\n",
    "        self.inference_mode = inference_mode\n",
    "        self.theta = None\n",
    "        self.b = None\n",
    "        self.name = name\n",
    "\n",
    "    def _output(self, theta, b, inputs, h):\n",
    "        xh = tf.concat([inputs, h], 1)\n",
    "        return tf.matmul(xh, theta) + tf.squeeze(b)\n",
    "    \n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\n",
    "        Args:\n",
    "         inputs: `2-D` tensor with shape `[batch_size x input_size]`.\n",
    "         state: An `LSTMStateTuple` of state tensors, each shaped\n",
    "            `[batch_size x self.state_size]`, if `state_is_tuple` has been set to\n",
    "            `True`. Otherwise, a `Tensor` shaped\n",
    "            `[batch_size x 2 * self.state_size]`.\n",
    "        Returns:\n",
    "         A pair containing the new hidden state, and the new state (either a\n",
    "            `LSTMStateTuple` or a concatenated state, depending on\n",
    "            `state_is_tuple`).\n",
    "        \"\"\"\n",
    "        # get noisy weights\n",
    "        if self.theta is None:\n",
    "            # Fetch initialization params from prior\n",
    "            rho_min_init, rho_max_init = self.prior.lstm_init()\n",
    "            self.theta = get_noisy_weights(shape=(self.n_unit_pre + self.n_unit, 4 * self.n_unit),\n",
    "                                           name=self.name + '_theta',\n",
    "                                           prior=self.prior,\n",
    "                                           is_training=self.is_training,\n",
    "                                           rho_min_init=rho_min_init,\n",
    "                                           rho_max_init=rho_max_init)\n",
    "            if self.bias:\n",
    "                self.b = get_noisy_weights(shape=(4 * self.n_unit, 1),\n",
    "                                           name=self.name + '_b',\n",
    "                                           prior=self.prior,\n",
    "                                           is_training=self.is_training,\n",
    "                                           rho_min_init=rho_min_init,\n",
    "                                           rho_max_init=rho_max_init)\n",
    "            else:\n",
    "                self.b = tf.get_variable(self.name + '_b', (4 * self.n_unit, 1), tf.float32,\n",
    "                                         tf.constant_initializer(0.))\n",
    "\n",
    "        # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "        if self._state_is_tuple:\n",
    "            c, h = state\n",
    "        else:\n",
    "            c, h = tf.split(value=state, num_or_size_splits=2, axis=1)\n",
    "\n",
    "        concat = self._output(self.theta, self.b, inputs, h)\n",
    "\n",
    "        # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "        i, j, f, o = tf.split(value=concat, num_or_size_splits=4, axis=1)\n",
    "\n",
    "        new_c = (\n",
    "            c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) * self._activation(j))\n",
    "        new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "        if self._state_is_tuple:\n",
    "            new_state = LSTMStateTuple(c=new_c, h=new_h)\n",
    "        else:\n",
    "            new_state = tf.concat(values=[new_c, new_h], axis=1)\n",
    "\n",
    "        return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScaleMixturePrior:\n",
    "    \"\"\"Build scale mixture prior with given configuration\n",
    "    1. A softplus is on top of the given rho's.\n",
    "    2. Restrict sample min max to prevent extreme values.\n",
    "    \"\"\"\n",
    "    # zero mean, mixture of two variances\n",
    "    def __init__(self):\n",
    "        self.pi = pi\n",
    "        self.rho_1 = rho_1\n",
    "        self.rho_2 = rho_2\n",
    "        self.sigma_1 = tf.nn.softplus(rho_1) + 1e-8\n",
    "        self.sigma_2 = tf.nn.softplus(rho_2) + 1e-8\n",
    "        self.sigma_mix = np.sqrt(0.5 * np.square(1.) + \\\n",
    "            (1. - 0.5) * np.square(0.5))\n",
    "\n",
    "    def lstm_init(self):\n",
    "        rho_max_init = math.log(math.exp(self.sigma_mix / 2.) - 1.)\n",
    "        rho_min_init = math.log(math.exp(self.sigma_mix / 4.) - 1.)\n",
    "        return rho_min_init, rho_max_init\n",
    "\n",
    "    def normal_init(self):\n",
    "        rho_max_init = math.log(math.exp(self.sigma_mix / 1.) - 1.)\n",
    "        rho_min_init = math.log(math.exp(self.sigma_mix / 2.) - 1.)\n",
    "        return rho_min_init, rho_max_init        \n",
    "\n",
    "\n",
    "def get_noisy_weights(shape, name, prior, is_training, rho_min_init=None,\n",
    "    rho_max_init=None):\n",
    "    \"\"\"Get noisy weights\n",
    "    1. Sample weights as given shape and configuration\n",
    "    2. Update histogram summary\n",
    "    3. Update KLqp\n",
    "    4. Return distribution of weights variables\n",
    "    \"\"\"\n",
    "    # add mean\n",
    "    with tf.variable_scope('BBB', reuse=not is_training):\n",
    "        mu = tf.get_variable(name + '_mean', shape, dtype=tf.float32)\n",
    "\n",
    "    # add rho\n",
    "    if rho_min_init is None or rho_max_init is None:\n",
    "        rho_min_init, rho_max_init = prior.lstm_init()\n",
    "    rho_init = tf.random_uniform_initializer(rho_min_init, rho_max_init)\n",
    "    with tf.variable_scope('BBB', reuse=not is_training):\n",
    "        rho = tf.get_variable(name + '_rho', shape, dtype=tf.float32,\n",
    "            initializer=rho_init)\n",
    "\n",
    "    # control output\n",
    "    if is_training or inference_mode == 'sample':\n",
    "        epsilon = Normal(0., 1.).sample(shape)\n",
    "        sigma = tf.nn.softplus(rho) + 1e-8\n",
    "        w = mu + sigma * epsilon\n",
    "    else:\n",
    "        w = mu\n",
    "\n",
    "    if is_training == False:\n",
    "        return w\n",
    "\n",
    "    # create histogram\n",
    "    tf.summary.histogram(name + '_mu_hist', mu)\n",
    "    tf.summary.histogram(name + '_sigma_hist', sigma)\n",
    "    tf.summary.histogram(name + '_rho_hist', rho)\n",
    "\n",
    "    # KL\n",
    "    kl = KL_scale_mixture(shape, tf.reshape(mu, [-1]), tf.reshape(sigma, [-1]),\n",
    "                          prior, w)\n",
    "    tf.add_to_collection('KL_layers', kl)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_state_tuples(state_tuples, name):\n",
    "    for state_tuple in state_tuples:\n",
    "        tf.add_to_collection(name, state_tuple.c)\n",
    "        tf.add_to_collection(name, state_tuple.h)\n",
    "\n",
    "def with_prefix(prefix, name):\n",
    "    \"\"\"Adds prefix to name.\"\"\"\n",
    "    return \"/\".join((prefix, name))\n",
    "\n",
    "def import_state_tuples(state_tuples, name, num_replicas):\n",
    "    restored = []\n",
    "    for i in range(len(state_tuples) * num_replicas):\n",
    "        c = tf.get_collection_ref(name)[2 * i + 0]\n",
    "        h = tf.get_collection_ref(name)[2 * i + 1]\n",
    "        restored.append(tf.contrib.rnn.LSTMStateTuple(c, h))\n",
    "    return tuple(restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BayesByBackprop(object):\n",
    "\n",
    "    def __init__(self, is_training, X, y):\n",
    "        self._is_training = is_training\n",
    "        self._rnn_params = None\n",
    "        self._cell = None\n",
    "        self.batch_size = 200\n",
    "        self.seq_length = 5\n",
    "        self.X = X\n",
    "        \n",
    "        if is_training == True:\n",
    "            n_batch = n_batch_train\n",
    "        else:\n",
    "            n_batch = n_batch_test\n",
    "\n",
    "        # Construct prior\n",
    "        prior = ScaleMixturePrior()\n",
    "\n",
    "        n_unit_pre = n_feature\n",
    "        # create 2 LSTMCells\n",
    "        rnn_layers = []\n",
    "        n_unit_pre = n_feature\n",
    "        for i in range(n_layer):\n",
    "            rnn_layers.append(BayesianLSTM(n_unit_pre, layers[i], prior, is_training,\n",
    "                                      inference_mode=inference_mode,\n",
    "                                      forget_bias=0.0,\n",
    "                                      name='bbb_lstm_{}'.format(i),\n",
    "                                      bias=True))\n",
    "            n_unit_pre = layers[i]\n",
    "\n",
    "        multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "        self._initial_state = multi_rnn_cell.zero_state(batch_size, tf.float32)\n",
    "        state = self._initial_state\n",
    "\n",
    "        # 'output' is a tensor of shape [batch_size, seq_length, n_feature]\n",
    "        # 'state' is a N-tuple where N is the number of LSTMCells containing a\n",
    "        # tf.contrib.rnn.LSTMStateTuple for each cell\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                               inputs=X,\n",
    "                                               time_major=False,\n",
    "                                               dtype=tf.float32)\n",
    "\n",
    "        # output layer\n",
    "        # add weight term\n",
    "        rho_min_init, rho_max_init = prior.normal_init()\n",
    "        if bias:\n",
    "            w = get_noisy_weights((50, 1), 'w', prior, is_training, rho_min_init, rho_max_init)\n",
    "        else:\n",
    "            w = tf.get_variable('w', (50, 1), tf.float32, tf.constant_initializer(0.))\n",
    "\n",
    "        # add bias term\n",
    "        if bias:\n",
    "            b = get_noisy_weights((1), 'b', prior, is_training, rho_min_init, rho_max_init)\n",
    "        else:\n",
    "            b = tf.get_variable('b', (1), tf.float32, tf.constant_initializer(0.))\n",
    "\n",
    "        output = tf.reshape(tf.matmul(outputs[:,seq_length-1,:], w) + b, [-1])\n",
    "\n",
    "        y = tf.reshape(y, [-1])\n",
    "        y_pred = Normal(output, 1.)\n",
    "        print(\"Finish predicting y\")\n",
    "\n",
    "        # Use the contrib sequence loss and average over the batches\n",
    "        loss = - tf.log(y_pred.prob(y) + 1e-8)\n",
    "\n",
    "        # Update the cost\n",
    "        self._cost = tf.reduce_sum(loss) / batch_size\n",
    "        self._final_state = state\n",
    "        \n",
    "        # 1. For testing, no kl term, just loss\n",
    "        self._kl_div = 0.\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # 2. For training, compute kl scaled by 1./n_batch\n",
    "        # Add up all prior's kl values\n",
    "        kl_div = tf.add_n(tf.get_collection('KL_layers'), 'kl_divergence')\n",
    "        # Compute ELBO\n",
    "        kl_const = 1. / n_batch\n",
    "        self._kl_div = kl_div * kl_const\n",
    "        self._total_loss = self._cost + self._kl_div\n",
    "        # Optimization:\n",
    "        ## Learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        ## Update all weights with gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._total_loss, tvars),\n",
    "                                          max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        ## Learning rate update\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "        print(\"Finish building model\")\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "    def export_ops(self, name):\n",
    "        \"\"\"Exports ops to collections.\"\"\"\n",
    "        self._name = name\n",
    "        ops = {with_prefix(self._name, \"cost\"): self._cost,\n",
    "               with_prefix(self._name, \"kl_div\"): self._kl_div}\n",
    "        if self._is_training:\n",
    "            ops.update(lr=self._lr, new_lr=self._new_lr, lr_update=self._lr_update)\n",
    "            if self._rnn_params:\n",
    "                ops.update(rnn_params=self._rnn_params)\n",
    "        for name, op in ops.items():\n",
    "            tf.add_to_collection(name, op)\n",
    "        self._initial_state_name = with_prefix(self._name, \"initial\")\n",
    "        self._final_state_name = with_prefix(self._name, \"final\")\n",
    "        export_state_tuples(self._initial_state, self._initial_state_name)\n",
    "        export_state_tuples(self._final_state, self._final_state_name)\n",
    "\n",
    "    def import_ops(self):\n",
    "        \"\"\"Imports ops from collections.\"\"\"\n",
    "        if self._is_training:\n",
    "            self._train_op = tf.get_collection_ref(\"train_op\")[0]\n",
    "            self._lr = tf.get_collection_ref(\"lr\")[0]\n",
    "            self._new_lr = tf.get_collection_ref(\"new_lr\")[0]\n",
    "            self._lr_update = tf.get_collection_ref(\"lr_update\")[0]\n",
    "            rnn_params = tf.get_collection_ref(\"rnn_params\")\n",
    "            if self._cell and rnn_params:\n",
    "                params_saveable = tf.contrib.cudnn_rnn.RNNParamsSaveable(\n",
    "                    self._cell,\n",
    "                    self._cell.params_to_canonical,\n",
    "                    self._cell.canonical_to_params,\n",
    "                    rnn_params,\n",
    "                    base_variable_scope=\"Model/RNN\")\n",
    "                tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, params_saveable)\n",
    "        self._cost = tf.get_collection_ref(with_prefix(self._name, \"cost\"))[0]\n",
    "        self._kl_div = tf.get_collection_ref(with_prefix(self._name, \"kl_div\"))[0]\n",
    "        num_replicas = 1\n",
    "        self._initial_state = import_state_tuples(\n",
    "            self._initial_state, self._initial_state_name, num_replicas)\n",
    "        self._final_state = import_state_tuples(\n",
    "            self._final_state, self._final_state_name, num_replicas)\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def initial_state_name(self):\n",
    "        return self._initial_state_name\n",
    "\n",
    "    @property\n",
    "    def final_state_name(self):\n",
    "        return self._final_state_name\n",
    "\n",
    "    @property\n",
    "    def kl_div(self):\n",
    "        return self._kl_div if self._is_training else tf.constant(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    \"\"\"Run the model on the given data\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    state = session.run(model.initial_state)\n",
    "\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "\n",
    "    n_batch = n_batch_train\n",
    "\n",
    "    if eval_op is not None:\n",
    "        n_batch = n_batch_test\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "        fetches[\"kl_divergence\"] = model.kl_div\n",
    "\n",
    "    for step in range(n_batch):\n",
    "        feed_dict = {}\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "\n",
    "        costs += cost\n",
    "\n",
    "        if verbose and (n_batch % 10 == 0):\n",
    "            print(\"%.3f batch loss: %.3f speed: %.0f second per batch\" %\n",
    "                  (n_batch, cost, (time.time() - start_time)/n_batch)\n",
    "                 )\n",
    "\n",
    "            if model._is_training:\n",
    "                print(\"KL is {}\".format(vals[\"kl_divergence\"]))\n",
    "\n",
    "    normal_constant = .5 * np.log(2 * np.pi * sigma_mix ** 2)\n",
    "    RMSE = np.sqrt((costs / n_batch - normal_constant) * sigma_mix **2)\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish predicting y\n",
      "Finish building model\n",
      "Finish building train model\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            X_train_batch, y_train_batch = pm_producer(X_train, y_train, batch_size)\n",
    "            m = BayesByBackprop(is_training=True, X=X_train_batch, y=y_train_batch)\n",
    "        tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "        tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "    print(\"Finish building train model\")\n",
    "\n",
    "    with tf.name_scope(\"Test\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            X_test_batch, y_test_batch = pm_producer(X_test, y_test, batch_size)\n",
    "            mtest = BayesByBackprop(is_training=False, X=X_test_batch, y=y_test_batch)\n",
    "    print(\"Finish building test model\")\n",
    "\n",
    "    models = {\"Train\": m, \"Test\": mtest}\n",
    "    for name, model in models.items():\n",
    "        model.export_ops(name)\n",
    "    metagraph = tf.train.export_meta_graph()\n",
    "    soft_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/saved_new/model.ckpt-0\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path ./model/saved_new/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, The node 'Merge/MergeSummary' has inputs from different frames. The input 'Train/Model/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/bayesian_lstm/bbb_lstm_1_b_rho_hist' is in frame 'Train/Model/rnn/while/Train/Model/rnn/while/'. The input 'Train/Learning_Rate' is in frame ''.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "Epoch: 1 Learning rate: 0.010\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "The node 'Merge/MergeSummary' has inputs from different frames. The input 'Train/Model/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/bayesian_lstm/bbb_lstm_1_b_rho_hist' is in frame 'Train/Model/rnn/while/Train/Model/rnn/while/'. The input 'Train/Learning_Rate' is in frame ''.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: The node 'Train/Model/kl_divergence' has inputs from different frames. The input 'Train/Model/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/bayesian_lstm/add_9' is in frame 'Train/Model/rnn/while/Train/Model/rnn/while/'. The input 'Train/Model/add_13' is in frame ''.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m    953\u001b[0m           start_standard_services=start_standard_services)\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-58d50d03f473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %d Learning rate: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtrain_RMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %d Train loss: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_RMSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c44fb275d95c>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(session, model, eval_op, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cost\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: The node 'Train/Model/kl_divergence' has inputs from different frames. The input 'Train/Model/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/bayesian_lstm/add_9' is in frame 'Train/Model/rnn/while/Train/Model/rnn/while/'. The input 'Train/Model/add_13' is in frame ''.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-58d50d03f473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving model to %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't stop after throw()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;31m# threads which are not checking for `should_stop()`.  They\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;31m# will be stopped when we close the session further down.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Close the session to finish up all pending calls.  We do not care\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, threads, close_summary_writer)\u001b[0m\n\u001b[1;32m    790\u001b[0m       \u001b[0;31m# reported.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m       self._coord.join(threads,\n\u001b[0;32m--> 792\u001b[0;31m                        stop_grace_period_secs=self._stop_grace_secs)\n\u001b[0m\u001b[1;32m    793\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m       \u001b[0;31m# Close the writer last, in case one of the running threads was using it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mstragglers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_live_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mstop_on_exception\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_timer_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m           \u001b[0mnext_timer_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer_interval_secs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m       summary_strs, global_step = self._sess.run([self._sv.summary_op,\n\u001b[0;32m--> 994\u001b[0;31m                                                   self._sv.global_step])\n\u001b[0m\u001b[1;32m    995\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       \u001b[0msummary_strs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: The node 'Merge/MergeSummary' has inputs from different frames. The input 'Train/Model/rnn/while/rnn/multi_rnn_cell/cell_1/cell_1/bayesian_lstm/bbb_lstm_1_b_rho_hist' is in frame 'Train/Model/rnn/while/Train/Model/rnn/while/'. The input 'Train/Learning_Rate' is in frame ''."
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    tf.train.import_meta_graph(metagraph)\n",
    "    for model in models.values():\n",
    "        model.import_ops()\n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    config_proto = tf.ConfigProto(allow_soft_placement=soft_placement)\n",
    "    with sv.managed_session(config=config_proto) as session:\n",
    "        for i in range(n_epoch):\n",
    "            lr_decay = lr_decay ** max(i + 1 - n_epoch_decay, 0.0)\n",
    "            m.assign_lr(session, learning_rate * lr_decay)\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            train_RMSE = run_epoch(session, m, eval_op=m.train_op, verbose=True)\n",
    "            print(\"Epoch: %d Train loss: %.3f\" % (i + 1, train_RMSE))\n",
    "        test_RMSE = run_epoch(session, mtest, eval_op=None)\n",
    "        print(\"Test loss: %.3f\" % test_RMSE)\n",
    "        if save_path:\n",
    "            print(\"Saving model to %s.\" % save_path)\n",
    "            sv.saver.save(session, save_path, global_step=sv.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
